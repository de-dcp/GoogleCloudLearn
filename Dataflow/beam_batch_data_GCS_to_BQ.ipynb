{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299c2c4b-0ff2-4c4e-9eca-4e292acb0a21",
   "metadata": {},
   "source": [
    "#Command to trigger dataflow\n",
    "\n",
    "python apache_beam_batch_job_gcs_to_bq.py \\\n",
    "        --inpu 'gs://test-project-dcp/sample_data/customer_purchasing_behaviors.csv'\\\n",
    "        --output_table 'lexical-cider-440507-u0:test_dcp.test_table'\\\n",
    "        --project 'lexical-cider-440507-u0'\\\n",
    "        --temp_location 'gs://test-project-dcp/temp'\\\n",
    "        --staging_location 'gs://test-project-dcp/temp'\\\n",
    "        --region 'us-central1'\\\n",
    "        --runner 'DataflowRunner'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbab87b-3dd0-49e4-9106-710fd92a0973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.io.gcp.bigquery import WriteToBigQuery\n",
    "import argparse\n",
    "\n",
    "class convert_dict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        temp_dict = {}\n",
    "        temp_dict['user_id']=element[0]\n",
    "        temp_dict['age']=element[1]\n",
    "        temp_dict['annual_income']=element[2]\n",
    "        temp_dict['purchase_amount']=element[3]\n",
    "        temp_dict['loyalty_score']=element[4]\n",
    "        temp_dict['region']=element[5]\n",
    "        temp_dict['purchase_frequency']=element[6]\n",
    "        yield temp_dict\n",
    "\n",
    "def run():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input', dest='input', default='gs://test-project-dcp/sample_data/customer_purchasing_behaviors.csv', help='Input file to process.')\n",
    "    parser.add_argument('--output_table', dest='output_table', default='lexical-cider-440507-u0:test_dcp.test_table', help='output table to process.')\n",
    "    \n",
    "    known_args, pipeline_args = parser.parse_known_args()\n",
    "    options=PipelineOptions(pipeline_args)\n",
    "\n",
    "    # Create the pipeline\n",
    "    with beam.Pipeline(options=options) as p:\n",
    "        # Read data from GCS\n",
    "        data = (\n",
    "            p\n",
    "            | 'ReadFromGCS' >> beam.io.ReadFromText(known_args.input, skip_header_lines=1)\n",
    "            | 'split_map' >> beam.Map(lambda x: x.split(','))\n",
    "            | 'filter_values' >> beam.Filter(lambda x: int(x[1]) > 20)\n",
    "            | 'convert_dict' >> beam.ParDo(convert_dict()) # Parse the CSV into dictionaries\n",
    "            #| 'print' >> beam.Map(print)\n",
    "        )\n",
    "\n",
    "        # Write the parsed data to BigQuery\n",
    "        data | 'WriteToBigQuery' >> WriteToBigQuery(\n",
    "            known_args.output_table,  # BigQuery table name\n",
    "            schema='user_id:INTEGER,age:INTEGER,annual_income:FLOAT,purchase_amount:FLOAT,loyalty_score:FLOAT,region:STRING,purchase_frequency:INTEGER',\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, # Let BigQuery auto-detect the schema\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE  # Append data to BigQuery\n",
    "        )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
